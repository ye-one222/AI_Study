{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05adb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f0f59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### LeNet-5 구조 정의 - default (배치 정규화)\n",
    "### 활성화 함수: tanh, 출력층 활성화 함수: softmax, 최적화: Adam, 손실함수: 교차엔트로피\n",
    "##########################\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # Sequential: layer을 순서대로 적용\n",
    "        self.feature_extractor = nn.Sequential(          \n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(120),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=n_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0215090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋 로드\n",
    "\n",
    "def load_data(validation_split=0.2, batch_size=32):\n",
    "    transform = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "    dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "    train_idx, val_idx = train_test_split(np.arange(len(dataset)), test_size=validation_split, random_state=42, shuffle=True)\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = load_data(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62fc1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X, y_true in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        y_hat, _ = model(X)\n",
    "        loss = criterion(y_hat, y_true)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y_true in val_loader:\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "            y_hat, _ = model(X)\n",
    "            loss = criterion(y_hat, y_true)\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    return model, epoch_loss\n",
    "\n",
    "def get_accuracy(model, data_loader, device):\n",
    "    correct_pred = 0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "    return correct_pred.float() / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93079ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: Learning Rate=0.001411275245141201, Batch Size=32, Validation Accuracy=0.9900833368301392\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def hyperparameter_optimization(trials=10):\n",
    "    best_val_acc = 0\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    for _ in range(trials):\n",
    "        lr = 10 ** np.random.uniform(-4, -2)\n",
    "        batch_size = random.choice([16, 32, 64, 128])\n",
    "\n",
    "        train_loader, val_loader = load_data(batch_size=batch_size)\n",
    "        model = LeNet5(10).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(10):  # Modify the number of epochs as needed\n",
    "            model, optimizer, train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "            model, val_loss = validate_epoch(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        val_acc = get_accuracy(model, val_loader, DEVICE).item()\n",
    "        results.append((lr, batch_size, val_acc))\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = (lr, batch_size)\n",
    "\n",
    "    print(f\"Best params: Learning Rate={best_params[0]}, Batch Size={best_params[1]}, Validation Accuracy={best_val_acc}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the hyperparameter optimization\n",
    "results = hyperparameter_optimization(trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2852c0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef plot_results(results):\\n    lrs, batch_sizes, val_accs = zip(*results)\\n    plt.figure(figsize=(12, 6))\\n\\n    plt.subplot(1, 2, 1)\\n    plt.scatter(lrs, val_accs)\\n    plt.xscale('log')\\n    plt.xlabel('Learning Rate')\\n    plt.ylabel('Validation Accuracy')\\n    plt.title('Learning Rate vs Validation Accuracy')\\n\\n    plt.subplot(1, 2, 2)\\n    plt.scatter(batch_sizes, val_accs)\\n    plt.xlabel('Batch Size')\\n    plt.ylabel('Validation Accuracy')\\n    plt.title('Batch Size vs Validation Accuracy')\\n\\n    plt.show()\\n\\n# Plot the results\\nplot_results(results)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def plot_results(results):\n",
    "    lrs, batch_sizes, val_accs = zip(*results)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(lrs, val_accs)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Learning Rate vs Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(batch_sizes, val_accs)\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Batch Size vs Validation Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_results(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2db34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
