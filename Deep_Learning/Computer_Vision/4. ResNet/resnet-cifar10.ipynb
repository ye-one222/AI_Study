{"cells":[{"cell_type":"code","source":["# [참고] https://cryptosalamander.tistory.com/156"],"metadata":{"id":"7mC364RXEwwh"},"id":"7mC364RXEwwh","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"90b75e60","metadata":{"id":"90b75e60"},"outputs":[],"source":["import torchvision\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","#from resnet import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n","import os\n","import torchvision.models as models\n","import torchsummary"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Residual Block 구조 정의\n","class BasicBlock(nn.Module):\n","    mul = 1\n","    def __init__(self, in_planes, out_planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","\n","        # stride를 통해 너비와 높이 조정\n","        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_planes)\n","\n","        # stride = 1, padding = 1이므로, 너비와 높이는 항시 유지됨\n","        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_planes)\n","\n","        # x를 그대로 더해주기 위함\n","        self.shortcut = nn.Sequential()\n","\n","        # 만약 size가 안맞아 합연산이 불가하다면, 연산 가능하도록 모양을 맞춰줌\n","        if stride != 1: # x와\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_planes)\n","            )\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = F.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out += self.shortcut(x) # 필요에 따라 layer를 Skip\n","        out = F.relu(out)\n","        return out\n","\n","class BottleNeck(nn.Module):\n","    mul = 4\n","    def __init__(self, in_planes, out_planes, stride=1):\n","        super(BottleNeck, self).__init__()\n","\n","        #첫 Convolution은 너비와 높이 downsampling\n","        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_planes)\n","\n","        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_planes)\n","\n","        self.conv3 = nn.Conv2d(out_planes, out_planes*self.mul, kernel_size=1, stride=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_planes*self.mul)\n","\n","        self.shortcut = nn.Sequential()\n","\n","        if stride != 1 or in_planes != out_planes*self.mul:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, out_planes*self.mul, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_planes*self.mul)\n","            )\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = F.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = F.relu(out)\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        #RGB 3개채널에서 64개의 Kernel 사용\n","        self.in_planes = 64\n","\n","        # Resnet 논문 구조 그대로 구현\n","        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=7, stride=2, padding = 3)\n","        self.bn1 = nn.BatchNorm2d(self.in_planes)\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n","        self.linear = nn.Linear(512 * block.mul, num_classes)\n","\n","    def make_layer(self, block, out_planes, num_blocks, stride):\n","        # layer 앞부분에서만 크기를 절반으로 줄이므로, 아래와 같은 구조\n","        strides = [stride] + [1] * (num_blocks-1)\n","        layers = []\n","        for i in range(num_blocks):\n","            layers.append(block(self.in_planes, out_planes, strides[i]))\n","            self.in_planes = block.mul * out_planes\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = F.relu(out)\n","        out = self.maxpool1(out)\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.avgpool(out)\n","        out = torch.flatten(out,1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3, 4, 6, 3])\n","\n","def ResNet50():\n","    return ResNet(BottleNeck, [3, 4, 6, 3])\n","\n","def ResNet101():\n","    return ResNet(BottleNeck, [3, 4, 23, 3])\n","\n","def ResNet152():\n","    return ResNet(BottleNeck, [3, 8, 36, 3])"],"metadata":{"id":"lA2-Fu8VITmS"},"id":"lA2-Fu8VITmS","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b831f0d4","metadata":{"id":"b831f0d4"},"outputs":[],"source":["# Simple Learning Rate Scheduler\n","def lr_scheduler(optimizer, epoch):\n","    lr = learning_rate\n","    if epoch >= 50:\n","        lr /= 10\n","    if epoch >= 100:\n","        lr /= 10\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","# Xavier\n","def init_weights(m):\n","    if isinstance(m, nn.Linear):\n","        #torch.nn.init.xavier_uniform(m.weight)\n","        torch.nn.init.kaiming_uniform(m.weight)\n","        m.bias.data.fill_(0.01)"]},{"cell_type":"code","execution_count":null,"id":"52eadf28","metadata":{"id":"52eadf28","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723781430668,"user_tz":-540,"elapsed":10513,"user":{"displayName":"정예원","userId":"14152642193567389072"}},"outputId":"e6e54b07-539f-4036-a822-bc088db55e09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:05<00:00, 30922103.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=8)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=8)"]},{"cell_type":"code","execution_count":null,"id":"26f6f682","metadata":{"id":"26f6f682","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723781433448,"user_tz":-540,"elapsed":904,"user":{"displayName":"정예원","userId":"14152642193567389072"}},"outputId":"61faba1e-fb40-4f50-846d-1a8eccf46a12"},"outputs":[{"output_type":"stream","name":"stdout","text":["use_cuda :  True\n"]}],"source":["#device = 'cuda'\n","use_cuda = torch.cuda.is_available()\n","print(\"use_cuda : \", use_cuda)\n","FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","\n","model = ResNet101()\n","# ResNet18, ResNet34, ResNet50, ResNet101, ResNet152 중에 택일하여 사용"]},{"cell_type":"code","execution_count":null,"id":"1f5008c1","metadata":{"id":"1f5008c1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723781436791,"user_tz":-540,"elapsed":535,"user":{"displayName":"정예원","userId":"14152642193567389072"}},"outputId":"8a67d0a3-6585-4f74-e328-86002ca7707b"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-59d0c508d1b5>:15: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n","  torch.nn.init.kaiming_uniform(m.weight)\n"]}],"source":["model.apply(init_weights)\n","model = model.to(device)"]},{"cell_type":"code","source":["print(torchsummary.summary(model, (3, 32, 32)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0tKcwuIzSyK","executionInfo":{"status":"ok","timestamp":1723779996576,"user_tz":-540,"elapsed":1320,"user":{"displayName":"정예원","userId":"14152642193567389072"}},"outputId":"cf48b753-f801-447e-bd8d-109648722ce7"},"id":"V0tKcwuIzSyK","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 16, 16]           9,472\n","       BatchNorm2d-2           [-1, 64, 16, 16]             128\n","         MaxPool2d-3             [-1, 64, 8, 8]               0\n","            Conv2d-4             [-1, 64, 8, 8]           4,096\n","       BatchNorm2d-5             [-1, 64, 8, 8]             128\n","            Conv2d-6             [-1, 64, 8, 8]          36,864\n","       BatchNorm2d-7             [-1, 64, 8, 8]             128\n","            Conv2d-8            [-1, 256, 8, 8]          16,384\n","       BatchNorm2d-9            [-1, 256, 8, 8]             512\n","           Conv2d-10            [-1, 256, 8, 8]          16,384\n","      BatchNorm2d-11            [-1, 256, 8, 8]             512\n","       BottleNeck-12            [-1, 256, 8, 8]               0\n","           Conv2d-13             [-1, 64, 8, 8]          16,384\n","      BatchNorm2d-14             [-1, 64, 8, 8]             128\n","           Conv2d-15             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-16             [-1, 64, 8, 8]             128\n","           Conv2d-17            [-1, 256, 8, 8]          16,384\n","      BatchNorm2d-18            [-1, 256, 8, 8]             512\n","       BottleNeck-19            [-1, 256, 8, 8]               0\n","           Conv2d-20             [-1, 64, 8, 8]          16,384\n","      BatchNorm2d-21             [-1, 64, 8, 8]             128\n","           Conv2d-22             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-23             [-1, 64, 8, 8]             128\n","           Conv2d-24            [-1, 256, 8, 8]          16,384\n","      BatchNorm2d-25            [-1, 256, 8, 8]             512\n","       BottleNeck-26            [-1, 256, 8, 8]               0\n","           Conv2d-27            [-1, 128, 4, 4]          32,768\n","      BatchNorm2d-28            [-1, 128, 4, 4]             256\n","           Conv2d-29            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-30            [-1, 128, 4, 4]             256\n","           Conv2d-31            [-1, 512, 4, 4]          65,536\n","      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n","           Conv2d-33            [-1, 512, 4, 4]         131,072\n","      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n","       BottleNeck-35            [-1, 512, 4, 4]               0\n","           Conv2d-36            [-1, 128, 4, 4]          65,536\n","      BatchNorm2d-37            [-1, 128, 4, 4]             256\n","           Conv2d-38            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-39            [-1, 128, 4, 4]             256\n","           Conv2d-40            [-1, 512, 4, 4]          65,536\n","      BatchNorm2d-41            [-1, 512, 4, 4]           1,024\n","       BottleNeck-42            [-1, 512, 4, 4]               0\n","           Conv2d-43            [-1, 128, 4, 4]          65,536\n","      BatchNorm2d-44            [-1, 128, 4, 4]             256\n","           Conv2d-45            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-46            [-1, 128, 4, 4]             256\n","           Conv2d-47            [-1, 512, 4, 4]          65,536\n","      BatchNorm2d-48            [-1, 512, 4, 4]           1,024\n","       BottleNeck-49            [-1, 512, 4, 4]               0\n","           Conv2d-50            [-1, 128, 4, 4]          65,536\n","      BatchNorm2d-51            [-1, 128, 4, 4]             256\n","           Conv2d-52            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-53            [-1, 128, 4, 4]             256\n","           Conv2d-54            [-1, 512, 4, 4]          65,536\n","      BatchNorm2d-55            [-1, 512, 4, 4]           1,024\n","       BottleNeck-56            [-1, 512, 4, 4]               0\n","           Conv2d-57            [-1, 256, 2, 2]         131,072\n","      BatchNorm2d-58            [-1, 256, 2, 2]             512\n","           Conv2d-59            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-60            [-1, 256, 2, 2]             512\n","           Conv2d-61           [-1, 1024, 2, 2]         262,144\n","      BatchNorm2d-62           [-1, 1024, 2, 2]           2,048\n","           Conv2d-63           [-1, 1024, 2, 2]         524,288\n","      BatchNorm2d-64           [-1, 1024, 2, 2]           2,048\n","       BottleNeck-65           [-1, 1024, 2, 2]               0\n","           Conv2d-66            [-1, 256, 2, 2]         262,144\n","      BatchNorm2d-67            [-1, 256, 2, 2]             512\n","           Conv2d-68            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-69            [-1, 256, 2, 2]             512\n","           Conv2d-70           [-1, 1024, 2, 2]         262,144\n","      BatchNorm2d-71           [-1, 1024, 2, 2]           2,048\n","       BottleNeck-72           [-1, 1024, 2, 2]               0\n","           Conv2d-73            [-1, 256, 2, 2]         262,144\n","      BatchNorm2d-74            [-1, 256, 2, 2]             512\n","           Conv2d-75            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-76            [-1, 256, 2, 2]             512\n","           Conv2d-77           [-1, 1024, 2, 2]         262,144\n","      BatchNorm2d-78           [-1, 1024, 2, 2]           2,048\n","       BottleNeck-79           [-1, 1024, 2, 2]               0\n","           Conv2d-80            [-1, 256, 2, 2]         262,144\n","      BatchNorm2d-81            [-1, 256, 2, 2]             512\n","           Conv2d-82            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-83            [-1, 256, 2, 2]             512\n","           Conv2d-84           [-1, 1024, 2, 2]         262,144\n","      BatchNorm2d-85           [-1, 1024, 2, 2]           2,048\n","       BottleNeck-86           [-1, 1024, 2, 2]               0\n","           Conv2d-87            [-1, 256, 2, 2]         262,144\n","      BatchNorm2d-88            [-1, 256, 2, 2]             512\n","           Conv2d-89            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-90            [-1, 256, 2, 2]             512\n","           Conv2d-91           [-1, 1024, 2, 2]         262,144\n","      BatchNorm2d-92           [-1, 1024, 2, 2]           2,048\n","       BottleNeck-93           [-1, 1024, 2, 2]               0\n","           Conv2d-94            [-1, 256, 2, 2]         262,144\n","      BatchNorm2d-95            [-1, 256, 2, 2]             512\n","           Conv2d-96            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-97            [-1, 256, 2, 2]             512\n","           Conv2d-98           [-1, 1024, 2, 2]         262,144\n","      BatchNorm2d-99           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-100           [-1, 1024, 2, 2]               0\n","          Conv2d-101            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-102            [-1, 256, 2, 2]             512\n","          Conv2d-103            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-104            [-1, 256, 2, 2]             512\n","          Conv2d-105           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-106           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-107           [-1, 1024, 2, 2]               0\n","          Conv2d-108            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-109            [-1, 256, 2, 2]             512\n","          Conv2d-110            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-111            [-1, 256, 2, 2]             512\n","          Conv2d-112           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-113           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-114           [-1, 1024, 2, 2]               0\n","          Conv2d-115            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-116            [-1, 256, 2, 2]             512\n","          Conv2d-117            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-118            [-1, 256, 2, 2]             512\n","          Conv2d-119           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-120           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-121           [-1, 1024, 2, 2]               0\n","          Conv2d-122            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-123            [-1, 256, 2, 2]             512\n","          Conv2d-124            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-125            [-1, 256, 2, 2]             512\n","          Conv2d-126           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-127           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-128           [-1, 1024, 2, 2]               0\n","          Conv2d-129            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-130            [-1, 256, 2, 2]             512\n","          Conv2d-131            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-132            [-1, 256, 2, 2]             512\n","          Conv2d-133           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-134           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-135           [-1, 1024, 2, 2]               0\n","          Conv2d-136            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-137            [-1, 256, 2, 2]             512\n","          Conv2d-138            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-139            [-1, 256, 2, 2]             512\n","          Conv2d-140           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-141           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-142           [-1, 1024, 2, 2]               0\n","          Conv2d-143            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-144            [-1, 256, 2, 2]             512\n","          Conv2d-145            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-146            [-1, 256, 2, 2]             512\n","          Conv2d-147           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-148           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-149           [-1, 1024, 2, 2]               0\n","          Conv2d-150            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-151            [-1, 256, 2, 2]             512\n","          Conv2d-152            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-153            [-1, 256, 2, 2]             512\n","          Conv2d-154           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-155           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-156           [-1, 1024, 2, 2]               0\n","          Conv2d-157            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-158            [-1, 256, 2, 2]             512\n","          Conv2d-159            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-160            [-1, 256, 2, 2]             512\n","          Conv2d-161           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-162           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-163           [-1, 1024, 2, 2]               0\n","          Conv2d-164            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-165            [-1, 256, 2, 2]             512\n","          Conv2d-166            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-167            [-1, 256, 2, 2]             512\n","          Conv2d-168           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-169           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-170           [-1, 1024, 2, 2]               0\n","          Conv2d-171            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-172            [-1, 256, 2, 2]             512\n","          Conv2d-173            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-174            [-1, 256, 2, 2]             512\n","          Conv2d-175           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-176           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-177           [-1, 1024, 2, 2]               0\n","          Conv2d-178            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-179            [-1, 256, 2, 2]             512\n","          Conv2d-180            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-181            [-1, 256, 2, 2]             512\n","          Conv2d-182           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-183           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-184           [-1, 1024, 2, 2]               0\n","          Conv2d-185            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-186            [-1, 256, 2, 2]             512\n","          Conv2d-187            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-188            [-1, 256, 2, 2]             512\n","          Conv2d-189           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-190           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-191           [-1, 1024, 2, 2]               0\n","          Conv2d-192            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-193            [-1, 256, 2, 2]             512\n","          Conv2d-194            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-195            [-1, 256, 2, 2]             512\n","          Conv2d-196           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-197           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-198           [-1, 1024, 2, 2]               0\n","          Conv2d-199            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-200            [-1, 256, 2, 2]             512\n","          Conv2d-201            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-202            [-1, 256, 2, 2]             512\n","          Conv2d-203           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-204           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-205           [-1, 1024, 2, 2]               0\n","          Conv2d-206            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-207            [-1, 256, 2, 2]             512\n","          Conv2d-208            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-209            [-1, 256, 2, 2]             512\n","          Conv2d-210           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-211           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-212           [-1, 1024, 2, 2]               0\n","          Conv2d-213            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-214            [-1, 256, 2, 2]             512\n","          Conv2d-215            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-216            [-1, 256, 2, 2]             512\n","          Conv2d-217           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-218           [-1, 1024, 2, 2]           2,048\n","      BottleNeck-219           [-1, 1024, 2, 2]               0\n","          Conv2d-220            [-1, 512, 1, 1]         524,288\n","     BatchNorm2d-221            [-1, 512, 1, 1]           1,024\n","          Conv2d-222            [-1, 512, 1, 1]       2,359,296\n","     BatchNorm2d-223            [-1, 512, 1, 1]           1,024\n","          Conv2d-224           [-1, 2048, 1, 1]       1,048,576\n","     BatchNorm2d-225           [-1, 2048, 1, 1]           4,096\n","          Conv2d-226           [-1, 2048, 1, 1]       2,097,152\n","     BatchNorm2d-227           [-1, 2048, 1, 1]           4,096\n","      BottleNeck-228           [-1, 2048, 1, 1]               0\n","          Conv2d-229            [-1, 512, 1, 1]       1,048,576\n","     BatchNorm2d-230            [-1, 512, 1, 1]           1,024\n","          Conv2d-231            [-1, 512, 1, 1]       2,359,296\n","     BatchNorm2d-232            [-1, 512, 1, 1]           1,024\n","          Conv2d-233           [-1, 2048, 1, 1]       1,048,576\n","     BatchNorm2d-234           [-1, 2048, 1, 1]           4,096\n","      BottleNeck-235           [-1, 2048, 1, 1]               0\n","          Conv2d-236            [-1, 512, 1, 1]       1,048,576\n","     BatchNorm2d-237            [-1, 512, 1, 1]           1,024\n","          Conv2d-238            [-1, 512, 1, 1]       2,359,296\n","     BatchNorm2d-239            [-1, 512, 1, 1]           1,024\n","          Conv2d-240           [-1, 2048, 1, 1]       1,048,576\n","     BatchNorm2d-241           [-1, 2048, 1, 1]           4,096\n","      BottleNeck-242           [-1, 2048, 1, 1]               0\n","AdaptiveAvgPool2d-243           [-1, 2048, 1, 1]               0\n","          Linear-244                   [-1, 10]          20,490\n","================================================================\n","Total params: 42,520,714\n","Trainable params: 42,520,714\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 6.33\n","Params size (MB): 162.20\n","Estimated Total Size (MB): 168.54\n","----------------------------------------------------------------\n","None\n"]}]},{"cell_type":"code","execution_count":null,"id":"f811ed9b","metadata":{"id":"f811ed9b"},"outputs":[],"source":["#learning_rate = 0.1\n","learning_rate = 0.01\n","#num_epoch = 150\n","num_epoch = 160\n","model_name = 'model.pth'\n","\n","loss_fn = nn.CrossEntropyLoss()\n","#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_loss = 0\n","valid_loss = 0\n","correct = 0\n","total_cnt = 0\n","best_acc = 0"]},{"cell_type":"code","execution_count":null,"id":"2cd814ce","metadata":{"id":"2cd814ce","colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"status":"error","timestamp":1723781266311,"user_tz":-540,"elapsed":1643,"user":{"displayName":"정예원","userId":"14152642193567389072"}},"outputId":"05d629b2-58e1-4613-ac34-865240fa497c"},"outputs":[{"output_type":"stream","name":"stdout","text":["====== 1 epoch of 160 ======\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-289711d083d0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg_sq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amsgrad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0;31m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train (ResNet-101 with SGD)\n","for epoch in range(num_epoch):\n","    print(f\"====== { epoch+1} epoch of { num_epoch } ======\")\n","    model.train()\n","    lr_scheduler(optimizer, epoch)\n","    train_loss = 0\n","    valid_loss = 0\n","    correct = 0\n","    total_cnt = 0\n","    # Train Phase\n","    for step, batch in enumerate(train_loader):\n","        #  input and target\n","        batch[0], batch[1] = batch[0].to(device), batch[1].to(device)\n","        optimizer.zero_grad()\n","\n","        logits = model(batch[0])\n","        loss = loss_fn(logits, batch[1])\n","        loss.backward()\n","\n","        optimizer.step()\n","        train_loss += loss.item()\n","        _, predict = logits.max(1)\n","\n","        total_cnt += batch[1].size(0)\n","        correct +=  predict.eq(batch[1]).sum().item()\n","\n","        if step % 100 == 0 and step != 0:\n","            print(f\"\\n====== { step } Step of { len(train_loader) } ======\")\n","            print(f\"Train Acc : { correct / total_cnt }\")\n","            print(f\"Train Loss : { loss.item() / batch[1].size(0) }\")\n","\n","    correct = 0\n","    total_cnt = 0\n","\n","# Test Phase\n","    with torch.no_grad():\n","        model.eval()\n","        for step, batch in enumerate(test_loader):\n","            # input and target\n","            batch[0], batch[1] = batch[0].to(device), batch[1].to(device)\n","            total_cnt += batch[1].size(0)\n","            logits = model(batch[0])\n","            valid_loss += loss_fn(logits, batch[1])\n","            _, predict = logits.max(1)\n","            correct += predict.eq(batch[1]).sum().item()\n","        valid_acc = correct / total_cnt\n","        print(f\"\\nValid Acc : { valid_acc }\")\n","        print(f\"Valid Loss : { valid_loss / total_cnt }\")\n","\n","        if(valid_acc > best_acc):\n","            best_acc = valid_acc\n","            torch.save(model, model_name)\n","            print(\"Model Saved!\")"]},{"cell_type":"code","source":["print(\"Best accuracy : \", best_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0hfUSwnZZcN","executionInfo":{"status":"ok","timestamp":1723653164312,"user_tz":-540,"elapsed":524,"user":{"displayName":"정예원","userId":"14152642193567389072"}},"outputId":"433103e3-b912-48c6-b59e-ce56640e5886"},"id":"i0hfUSwnZZcN","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best accuracy :  0.6769\n"]}]},{"cell_type":"code","source":["# Train (ResNet-101 with Adam)\n","for epoch in range(num_epoch):\n","    print(f\"====== { epoch+1} epoch of { num_epoch } ======\")\n","    model.train()\n","    lr_scheduler(optimizer, epoch)\n","    train_loss = 0\n","    valid_loss = 0\n","    correct = 0\n","    total_cnt = 0\n","    # Train Phase\n","    for step, batch in enumerate(train_loader):\n","        #  input and target\n","        batch[0], batch[1] = batch[0].to(device), batch[1].to(device)\n","        optimizer.zero_grad()\n","\n","        logits = model(batch[0])\n","        loss = loss_fn(logits, batch[1])\n","        loss.backward()\n","\n","        optimizer.step()\n","        train_loss += loss.item()\n","        _, predict = logits.max(1)\n","\n","        total_cnt += batch[1].size(0)\n","        correct +=  predict.eq(batch[1]).sum().item()\n","\n","        if step % 100 == 0 and step != 0:\n","            print(f\"\\n====== { step } Step of { len(train_loader) } ======\")\n","            print(f\"Train Acc : { correct / total_cnt }\")\n","            print(f\"Train Loss : { loss.item() / batch[1].size(0) }\")\n","\n","    correct = 0\n","    total_cnt = 0\n","\n","# Test Phase\n","    with torch.no_grad():\n","        model.eval()\n","        for step, batch in enumerate(test_loader):\n","            # input and target\n","            batch[0], batch[1] = batch[0].to(device), batch[1].to(device)\n","            total_cnt += batch[1].size(0)\n","            logits = model(batch[0])\n","            valid_loss += loss_fn(logits, batch[1])\n","            _, predict = logits.max(1)\n","            correct += predict.eq(batch[1]).sum().item()\n","        valid_acc = correct / total_cnt\n","        print(f\"\\nValid Acc : { valid_acc }\")\n","        print(f\"Valid Loss : { valid_loss / total_cnt }\")\n","\n","        if(valid_acc > best_acc):\n","            best_acc = valid_acc\n","            torch.save(model, model_name)\n","            print(\"Model Saved!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUVDU4GHSP1f","executionInfo":{"status":"ok","timestamp":1723788765075,"user_tz":-540,"elapsed":7319349,"user":{"displayName":"정예원","userId":"14152642193567389072"}},"outputId":"b81c6e0f-8419-4750-f5f1-e199e46a8cee"},"id":"MUVDU4GHSP1f","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====== 1 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.10013149752475248\n","Train Loss : 0.00904519110918045\n","\n","Valid Acc : 0.1092\n","Valid Loss : 0.018186045810580254\n","Model Saved!\n","====== 2 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.12209931930693069\n","Train Loss : 0.01019605528563261\n","\n","Valid Acc : 0.1976\n","Valid Loss : 0.008371908217668533\n","Model Saved!\n","====== 3 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.20660581683168316\n","Train Loss : 0.008035014383494854\n","\n","Valid Acc : 0.1317\n","Valid Loss : 0.16955716907978058\n","====== 4 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.16464263613861385\n","Train Loss : 0.007626874838024378\n","\n","Valid Acc : 0.2259\n","Valid Loss : 0.008154439739882946\n","Model Saved!\n","====== 5 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.23743038366336633\n","Train Loss : 0.007130158133804798\n","\n","Valid Acc : 0.2678\n","Valid Loss : 0.007771091070026159\n","Model Saved!\n","====== 6 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.27583539603960394\n","Train Loss : 0.007436450105160475\n","\n","Valid Acc : 0.2715\n","Valid Loss : 0.0077545675449073315\n","Model Saved!\n","====== 7 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.31609684405940597\n","Train Loss : 0.006603250280022621\n","\n","Valid Acc : 0.3521\n","Valid Loss : 0.007362628821283579\n","Model Saved!\n","====== 8 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.37070699257425743\n","Train Loss : 0.006862425245344639\n","\n","Valid Acc : 0.3901\n","Valid Loss : 0.006432544440031052\n","Model Saved!\n","====== 9 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.39805074257425743\n","Train Loss : 0.006158088333904743\n","\n","Valid Acc : 0.4372\n","Valid Loss : 0.006074871867895126\n","Model Saved!\n","====== 10 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.4511525371287129\n","Train Loss : 0.005387529730796814\n","\n","Valid Acc : 0.4559\n","Valid Loss : 0.005916958209127188\n","Model Saved!\n","====== 11 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.4841042698019802\n","Train Loss : 0.005507525056600571\n","\n","Valid Acc : 0.4506\n","Valid Loss : 0.00600218353793025\n","====== 12 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.5096302599009901\n","Train Loss : 0.004724013153463602\n","\n","Valid Acc : 0.4983\n","Valid Loss : 0.0057244435884058475\n","Model Saved!\n","====== 13 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.5528310643564357\n","Train Loss : 0.0049380655400455\n","\n","Valid Acc : 0.5573\n","Valid Loss : 0.004882903769612312\n","Model Saved!\n","====== 14 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.5868270420792079\n","Train Loss : 0.005075816065073013\n","\n","Valid Acc : 0.5519\n","Valid Loss : 0.005033042281866074\n","====== 15 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.6079053217821783\n","Train Loss : 0.003922878764569759\n","\n","Valid Acc : 0.6\n","Valid Loss : 0.004486232530325651\n","Model Saved!\n","====== 16 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.6318842821782178\n","Train Loss : 0.004389597102999687\n","\n","Valid Acc : 0.6338\n","Valid Loss : 0.00428842194378376\n","Model Saved!\n","====== 17 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.6620513613861386\n","Train Loss : 0.0032082628458738327\n","\n","Valid Acc : 0.5849\n","Valid Loss : 0.0054270802065730095\n","====== 18 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.6736927599009901\n","Train Loss : 0.004146138671785593\n","\n","Valid Acc : 0.6721\n","Valid Loss : 0.003781120525673032\n","Model Saved!\n","====== 19 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.6907487623762376\n","Train Loss : 0.0032957426737993956\n","\n","Valid Acc : 0.6969\n","Valid Loss : 0.0035276939161121845\n","Model Saved!\n","====== 20 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.708384900990099\n","Train Loss : 0.003194856457412243\n","\n","Valid Acc : 0.6696\n","Valid Loss : 0.003961917944252491\n","====== 21 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7178604579207921\n","Train Loss : 0.0027064860332757235\n","\n","Valid Acc : 0.6545\n","Valid Loss : 0.0040258304215967655\n","====== 22 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7311649133663366\n","Train Loss : 0.0028990230057388544\n","\n","Valid Acc : 0.7181\n","Valid Loss : 0.003325891448184848\n","Model Saved!\n","====== 23 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.730159344059406\n","Train Loss : 0.0032025331165641546\n","\n","Valid Acc : 0.7254\n","Valid Loss : 0.0032205237075686455\n","Model Saved!\n","====== 24 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7447014232673267\n","Train Loss : 0.003378459485247731\n","\n","Valid Acc : 0.6799\n","Valid Loss : 0.0038052659947425127\n","====== 25 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7585473391089109\n","Train Loss : 0.002730558393523097\n","\n","Valid Acc : 0.7387\n","Valid Loss : 0.00314435176551342\n","Model Saved!\n","====== 26 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7588954207920792\n","Train Loss : 0.002413098933175206\n","\n","Valid Acc : 0.6938\n","Valid Loss : 0.003666490549221635\n","====== 27 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7669399752475248\n","Train Loss : 0.0023688862565904856\n","\n","Valid Acc : 0.7453\n","Valid Loss : 0.002987762214615941\n","Model Saved!\n","====== 28 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7726253094059405\n","Train Loss : 0.0023104650899767876\n","\n","Valid Acc : 0.7459\n","Valid Loss : 0.002990672132000327\n","Model Saved!\n","====== 29 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7797803217821783\n","Train Loss : 0.002302497625350952\n","\n","Valid Acc : 0.71\n","Valid Loss : 0.003640953451395035\n","====== 30 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.77734375\n","Train Loss : 0.002324063330888748\n","\n","Valid Acc : 0.7121\n","Valid Loss : 0.003487796289846301\n","====== 31 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7860844678217822\n","Train Loss : 0.0023421584628522396\n","\n","Valid Acc : 0.7478\n","Valid Loss : 0.002993230940774083\n","Model Saved!\n","====== 32 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7988474628712872\n","Train Loss : 0.002942364662885666\n","\n","Valid Acc : 0.7485\n","Valid Loss : 0.0030021388083696365\n","Model Saved!\n","====== 33 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8017868193069307\n","Train Loss : 0.0022774364333599806\n","\n","Valid Acc : 0.7466\n","Valid Loss : 0.002934106858447194\n","====== 34 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8035659034653465\n","Train Loss : 0.0018816218944266438\n","\n","Valid Acc : 0.7395\n","Valid Loss : 0.0031042122282087803\n","====== 35 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8061185024752475\n","Train Loss : 0.002039625309407711\n","\n","Valid Acc : 0.7033\n","Valid Loss : 0.0036810312885791063\n","====== 36 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8096766707920792\n","Train Loss : 0.002116398885846138\n","\n","Valid Acc : 0.6977\n","Valid Loss : 0.0037064594216644764\n","====== 37 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8113784034653465\n","Train Loss : 0.0022086859680712223\n","\n","Valid Acc : 0.7749\n","Valid Loss : 0.00275003002025187\n","Model Saved!\n","====== 38 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8163288985148515\n","Train Loss : 0.0018838770920410752\n","\n","Valid Acc : 0.7476\n","Valid Loss : 0.00300491601228714\n","====== 39 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8205445544554455\n","Train Loss : 0.0018644696101546288\n","\n","Valid Acc : 0.5632\n","Valid Loss : 0.00607701763510704\n","====== 40 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.7304300742574258\n","Train Loss : 0.0023784972727298737\n","\n","Valid Acc : 0.5677\n","Valid Loss : 0.012112976983189583\n","====== 41 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.779006806930693\n","Train Loss : 0.0020105342846363783\n","\n","Valid Acc : 0.6178\n","Valid Loss : 0.004915524739772081\n","====== 42 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8168703589108911\n","Train Loss : 0.0018602381460368633\n","\n","Valid Acc : 0.6895\n","Valid Loss : 0.0041785333305597305\n","====== 43 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8286664603960396\n","Train Loss : 0.002096779178828001\n","\n","Valid Acc : 0.7891\n","Valid Loss : 0.0025055031292140484\n","Model Saved!\n","====== 44 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8339650371287128\n","Train Loss : 0.001758115948177874\n","\n","Valid Acc : 0.7373\n","Valid Loss : 0.0032855826430022717\n","====== 45 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8348159034653465\n","Train Loss : 0.0014902148395776749\n","\n","Valid Acc : 0.7964\n","Valid Loss : 0.002457659225910902\n","Model Saved!\n","====== 46 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8407332920792079\n","Train Loss : 0.002011774806305766\n","\n","Valid Acc : 0.7739\n","Valid Loss : 0.002795096253976226\n","====== 47 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8382193688118812\n","Train Loss : 0.001725036185234785\n","\n","Valid Acc : 0.7257\n","Valid Loss : 0.003621197771281004\n","====== 48 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8439820544554455\n","Train Loss : 0.0017040098318830132\n","\n","Valid Acc : 0.7943\n","Valid Loss : 0.002450886182487011\n","====== 49 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.848855198019802\n","Train Loss : 0.0019506672397255898\n","\n","Valid Acc : 0.7717\n","Valid Loss : 0.0027360673993825912\n","====== 50 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8460705445544554\n","Train Loss : 0.0013332765083760023\n","\n","Valid Acc : 0.8018\n","Valid Loss : 0.002385932020843029\n","Model Saved!\n","====== 51 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8732595915841584\n","Train Loss : 0.0012696826597675681\n","\n","Valid Acc : 0.8462\n","Valid Loss : 0.0018182724015787244\n","Model Saved!\n","====== 52 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.89453125\n","Train Loss : 0.001283765072003007\n","\n","Valid Acc : 0.8535\n","Valid Loss : 0.0017813851591199636\n","Model Saved!\n","====== 53 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.8970064975247525\n","Train Loss : 0.0012476376723498106\n","\n","Valid Acc : 0.8523\n","Valid Loss : 0.0017850360600277781\n","====== 54 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9020730198019802\n","Train Loss : 0.0011979476548731327\n","\n","Valid Acc : 0.853\n","Valid Loss : 0.0017757329624146223\n","====== 55 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9022277227722773\n","Train Loss : 0.0009303115075454116\n","\n","Valid Acc : 0.8486\n","Valid Loss : 0.0018376718508079648\n","====== 56 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9057085396039604\n","Train Loss : 0.0013885009102523327\n","\n","Valid Acc : 0.8507\n","Valid Loss : 0.0018841821001842618\n","====== 57 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9078743811881188\n","Train Loss : 0.0012161772465333343\n","\n","Valid Acc : 0.858\n","Valid Loss : 0.001771534327417612\n","Model Saved!\n","====== 58 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9088412747524752\n","Train Loss : 0.0005791299045085907\n","\n","Valid Acc : 0.8492\n","Valid Loss : 0.001844304264523089\n","====== 59 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.915416150990099\n","Train Loss : 0.0010152228642255068\n","\n","Valid Acc : 0.8534\n","Valid Loss : 0.0018205135129392147\n","====== 60 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9174659653465347\n","Train Loss : 0.0011990061029791832\n","\n","Valid Acc : 0.8501\n","Valid Loss : 0.0018758102087303996\n","====== 61 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9149520420792079\n","Train Loss : 0.0009127940866164863\n","\n","Valid Acc : 0.8534\n","Valid Loss : 0.0018028529593721032\n","====== 62 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9183941831683168\n","Train Loss : 0.0009578253957442939\n","\n","Valid Acc : 0.853\n","Valid Loss : 0.0019024335779249668\n","====== 63 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9159576113861386\n","Train Loss : 0.0009941861499100924\n","\n","Valid Acc : 0.8539\n","Valid Loss : 0.001824252656660974\n","====== 64 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9209467821782178\n","Train Loss : 0.0009449970093555748\n","\n","Valid Acc : 0.8563\n","Valid Loss : 0.0018224000232294202\n","====== 65 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9246209777227723\n","Train Loss : 0.0010330119403079152\n","\n","Valid Acc : 0.8555\n","Valid Loss : 0.0018927825149148703\n","====== 66 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9246983292079208\n","Train Loss : 0.0011017817305400968\n","\n","Valid Acc : 0.8555\n","Valid Loss : 0.0018794328207150102\n","====== 67 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9274829826732673\n","Train Loss : 0.0009381893323734403\n","\n","Valid Acc : 0.8532\n","Valid Loss : 0.0019142957171425223\n","====== 68 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.925471844059406\n","Train Loss : 0.0008045991999097168\n","\n","Valid Acc : 0.8541\n","Valid Loss : 0.0019178801449015737\n","====== 69 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9274056311881188\n","Train Loss : 0.0009518941515125334\n","\n","Valid Acc : 0.8589\n","Valid Loss : 0.0018471040530130267\n","Model Saved!\n","====== 70 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9302676361386139\n","Train Loss : 0.000643429346382618\n","\n","Valid Acc : 0.8543\n","Valid Loss : 0.0019370007794350386\n","====== 71 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9302676361386139\n","Train Loss : 0.0007615248905494809\n","\n","Valid Acc : 0.856\n","Valid Loss : 0.0019116238690912724\n","====== 72 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9341352103960396\n","Train Loss : 0.0007266075117513537\n","\n","Valid Acc : 0.8535\n","Valid Loss : 0.001942464616149664\n","====== 73 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9330522896039604\n","Train Loss : 0.0008161980658769608\n","\n","Valid Acc : 0.8527\n","Valid Loss : 0.0019836442079395056\n","====== 74 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9372292698019802\n","Train Loss : 0.000872538483235985\n","\n","Valid Acc : 0.8558\n","Valid Loss : 0.001990358578041196\n","====== 75 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9367264851485149\n","Train Loss : 0.000661195837892592\n","\n","Valid Acc : 0.8552\n","Valid Loss : 0.0020338890608400106\n","====== 76 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9381188118811881\n","Train Loss : 0.0005004440317861736\n","\n","Valid Acc : 0.8501\n","Valid Loss : 0.0021216755267232656\n","====== 77 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9399752475247525\n","Train Loss : 0.0009092573309317231\n","\n","Valid Acc : 0.8565\n","Valid Loss : 0.002009635092690587\n","====== 78 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9390470297029703\n","Train Loss : 0.0008920785039663315\n","\n","Valid Acc : 0.854\n","Valid Loss : 0.002003357745707035\n","====== 79 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9410194925742574\n","Train Loss : 0.0006614391459152102\n","\n","Valid Acc : 0.8562\n","Valid Loss : 0.002092214999720454\n","====== 80 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9422957920792079\n","Train Loss : 0.0005526090390048921\n","\n","Valid Acc : 0.8572\n","Valid Loss : 0.0020416683983057737\n","====== 81 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9426825495049505\n","Train Loss : 0.0003846453328151256\n","\n","Valid Acc : 0.8588\n","Valid Loss : 0.0020130581688135862\n","====== 82 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9468208539603961\n","Train Loss : 0.000709374260623008\n","\n","Valid Acc : 0.8564\n","Valid Loss : 0.0020099731627851725\n","====== 83 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9466274752475248\n","Train Loss : 0.0003842487058136612\n","\n","Valid Acc : 0.8551\n","Valid Loss : 0.0021276758052408695\n","====== 84 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9470529084158416\n","Train Loss : 0.0005257584853097796\n","\n","Valid Acc : 0.8559\n","Valid Loss : 0.002085210056975484\n","====== 85 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9486386138613861\n","Train Loss : 0.0007760691805742681\n","\n","Valid Acc : 0.8545\n","Valid Loss : 0.00204469938762486\n","====== 86 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9497602103960396\n","Train Loss : 0.0006752118933945894\n","\n","Valid Acc : 0.8496\n","Valid Loss : 0.0021652746945619583\n","====== 87 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9488319925742574\n","Train Loss : 0.0003308784798718989\n","\n","Valid Acc : 0.8565\n","Valid Loss : 0.0021791746839880943\n","====== 88 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9513459158415841\n","Train Loss : 0.0007406690856441855\n","\n","Valid Acc : 0.8546\n","Valid Loss : 0.0021999450400471687\n","====== 89 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9501856435643564\n","Train Loss : 0.000718139111995697\n","\n","Valid Acc : 0.8512\n","Valid Loss : 0.002163911936804652\n","====== 90 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.950881806930693\n","Train Loss : 0.0005041186232119799\n","\n","Valid Acc : 0.8573\n","Valid Loss : 0.0021663049701601267\n","====== 91 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9539371905940595\n","Train Loss : 0.0004931985167786479\n","\n","Valid Acc : 0.8514\n","Valid Loss : 0.0022750175558030605\n","====== 92 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9516939975247525\n","Train Loss : 0.0005721385823562741\n","\n","Valid Acc : 0.8548\n","Valid Loss : 0.0022206425201147795\n","====== 93 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9561803836633663\n","Train Loss : 0.0003511033719405532\n","\n","Valid Acc : 0.8508\n","Valid Loss : 0.0023166905157268047\n","====== 94 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9533570544554455\n","Train Loss : 0.0007363281329162419\n","\n","Valid Acc : 0.854\n","Valid Loss : 0.0022902789060026407\n","====== 95 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.955987004950495\n","Train Loss : 0.000628022535238415\n","\n","Valid Acc : 0.8537\n","Valid Loss : 0.002243076218292117\n","====== 96 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9585782797029703\n","Train Loss : 0.0003140333283226937\n","\n","Valid Acc : 0.8511\n","Valid Loss : 0.002378596691414714\n","====== 97 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9564897896039604\n","Train Loss : 0.00043901707977056503\n","\n","Valid Acc : 0.8495\n","Valid Loss : 0.002311015035957098\n","====== 98 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9609761757425742\n","Train Loss : 0.00042834095074795187\n","\n","Valid Acc : 0.8547\n","Valid Loss : 0.0022961758077144623\n","====== 99 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9614789603960396\n","Train Loss : 0.00041103080729953945\n","\n","Valid Acc : 0.858\n","Valid Loss : 0.002260323613882065\n","====== 100 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9614402846534653\n","Train Loss : 0.0007448762189596891\n","\n","Valid Acc : 0.854\n","Valid Loss : 0.002321505220606923\n","====== 101 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.963644801980198\n","Train Loss : 0.00037874895497225225\n","\n","Valid Acc : 0.8599\n","Valid Loss : 0.0022032842971384525\n","Model Saved!\n","====== 102 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9684019183168316\n","Train Loss : 0.00027273775776848197\n","\n","Valid Acc : 0.8609\n","Valid Loss : 0.002219206653535366\n","Model Saved!\n","====== 103 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.968440594059406\n","Train Loss : 0.00038361470797099173\n","\n","Valid Acc : 0.8607\n","Valid Loss : 0.0022244236897677183\n","====== 104 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9690980816831684\n","Train Loss : 0.00037160495412535965\n","\n","Valid Acc : 0.8617\n","Valid Loss : 0.0022339187562465668\n","Model Saved!\n","====== 105 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9703357054455446\n","Train Loss : 0.0005127277108840644\n","\n","Valid Acc : 0.8614\n","Valid Loss : 0.002265718299895525\n","====== 106 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9707998143564357\n","Train Loss : 0.0003924418706446886\n","\n","Valid Acc : 0.8618\n","Valid Loss : 0.002275555394589901\n","Model Saved!\n","====== 107 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9708771658415841\n","Train Loss : 0.0003262606915086508\n","\n","Valid Acc : 0.8598\n","Valid Loss : 0.0022836935240775347\n","====== 108 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9720374381188119\n","Train Loss : 0.00032543836277909577\n","\n","Valid Acc : 0.8626\n","Valid Loss : 0.0022996794432401657\n","Model Saved!\n","====== 109 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9729269801980198\n","Train Loss : 0.0002488497702870518\n","\n","Valid Acc : 0.8609\n","Valid Loss : 0.002291328040882945\n","====== 110 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9726175742574258\n","Train Loss : 0.00021163441124372184\n","\n","Valid Acc : 0.8592\n","Valid Loss : 0.0023109589237719774\n","====== 111 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9736231435643564\n","Train Loss : 0.00028965179808437824\n","\n","Valid Acc : 0.8616\n","Valid Loss : 0.002282705157995224\n","====== 112 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.973700495049505\n","Train Loss : 0.0005655275308527052\n","\n","Valid Acc : 0.8599\n","Valid Loss : 0.0023195126559585333\n","====== 113 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9732750618811881\n","Train Loss : 0.00038563183625228703\n","\n","Valid Acc : 0.8623\n","Valid Loss : 0.002331193769350648\n","====== 114 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9747060643564357\n","Train Loss : 0.00025600133812986314\n","\n","Valid Acc : 0.8608\n","Valid Loss : 0.002368706511333585\n","====== 115 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9742032797029703\n","Train Loss : 0.0004274400416761637\n","\n","Valid Acc : 0.8628\n","Valid Loss : 0.002366392407566309\n","Model Saved!\n","====== 116 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9735457920792079\n","Train Loss : 0.00029209672356955707\n","\n","Valid Acc : 0.8627\n","Valid Loss : 0.0023582393769174814\n","====== 117 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9730043316831684\n","Train Loss : 0.00016694814257789403\n","\n","Valid Acc : 0.8629\n","Valid Loss : 0.002376738702878356\n","Model Saved!\n","====== 118 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.975363551980198\n","Train Loss : 0.0003428849158808589\n","\n","Valid Acc : 0.8616\n","Valid Loss : 0.002363139297813177\n","====== 119 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9742806311881188\n","Train Loss : 0.0003542899794410914\n","\n","Valid Acc : 0.862\n","Valid Loss : 0.0023991481866687536\n","====== 120 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9745900371287128\n","Train Loss : 0.00022603996330872178\n","\n","Valid Acc : 0.8605\n","Valid Loss : 0.0023940368555486202\n","====== 121 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9759050123762376\n","Train Loss : 0.0002737481554504484\n","\n","Valid Acc : 0.8615\n","Valid Loss : 0.0024458456318825483\n","====== 122 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.976253094059406\n","Train Loss : 0.00021832197671756148\n","\n","Valid Acc : 0.8608\n","Valid Loss : 0.002436756854876876\n","====== 123 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9749767945544554\n","Train Loss : 0.00029529473977163434\n","\n","Valid Acc : 0.8605\n","Valid Loss : 0.0024482395965605974\n","====== 124 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9764464727722773\n","Train Loss : 0.0002960456477012485\n","\n","Valid Acc : 0.8605\n","Valid Loss : 0.0024533383548259735\n","====== 125 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.975363551980198\n","Train Loss : 0.0003943504416383803\n","\n","Valid Acc : 0.8601\n","Valid Loss : 0.002450076164677739\n","====== 126 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9761370668316832\n","Train Loss : 0.00010882651258725673\n","\n","Valid Acc : 0.8591\n","Valid Loss : 0.0024688702542334795\n","====== 127 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9770652846534653\n","Train Loss : 0.00025777105474844575\n","\n","Valid Acc : 0.8594\n","Valid Loss : 0.002471220912411809\n","====== 128 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9769105816831684\n","Train Loss : 0.00041408362449146807\n","\n","Valid Acc : 0.8612\n","Valid Loss : 0.0024871923960745335\n","====== 129 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9774907178217822\n","Train Loss : 0.00036878976970911026\n","\n","Valid Acc : 0.8606\n","Valid Loss : 0.0024710139259696007\n","====== 130 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9765625\n","Train Loss : 0.0001228090113727376\n","\n","Valid Acc : 0.8612\n","Valid Loss : 0.0024840112309902906\n","====== 131 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9769879331683168\n","Train Loss : 0.00032986298901960254\n","\n","Valid Acc : 0.8615\n","Valid Loss : 0.0024916508700698614\n","====== 132 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9780321782178217\n","Train Loss : 0.00030531667289324105\n","\n","Valid Acc : 0.8608\n","Valid Loss : 0.0025059666950255632\n","====== 133 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9783415841584159\n","Train Loss : 9.453863458475098e-05\n","\n","Valid Acc : 0.8621\n","Valid Loss : 0.002533789025619626\n","====== 134 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9771426361386139\n","Train Loss : 0.0002785423130262643\n","\n","Valid Acc : 0.8596\n","Valid Loss : 0.002550580073148012\n","====== 135 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.977761448019802\n","Train Loss : 0.00018179015023633838\n","\n","Valid Acc : 0.8613\n","Valid Loss : 0.0025456822477281094\n","====== 136 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9788443688118812\n","Train Loss : 0.0002943554427474737\n","\n","Valid Acc : 0.8606\n","Valid Loss : 0.0025900278706103563\n","====== 137 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.977916150990099\n","Train Loss : 0.00017711834516376257\n","\n","Valid Acc : 0.8586\n","Valid Loss : 0.0025565691757947206\n","====== 138 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9780708539603961\n","Train Loss : 0.00017839271458797157\n","\n","Valid Acc : 0.8614\n","Valid Loss : 0.002570595359429717\n","====== 139 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9778774752475248\n","Train Loss : 0.00022420825553126633\n","\n","Valid Acc : 0.8605\n","Valid Loss : 0.002547604963183403\n","====== 140 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9792311262376238\n","Train Loss : 0.00021658497280441225\n","\n","Valid Acc : 0.8586\n","Valid Loss : 0.0025485740043222904\n","====== 141 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9785736386138614\n","Train Loss : 0.00021084744366817176\n","\n","Valid Acc : 0.8597\n","Valid Loss : 0.002592258621007204\n","====== 142 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9774133663366337\n","Train Loss : 0.00038643894367851317\n","\n","Valid Acc : 0.8603\n","Valid Loss : 0.002583785681053996\n","====== 143 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9790764232673267\n","Train Loss : 0.0001312012755079195\n","\n","Valid Acc : 0.8605\n","Valid Loss : 0.0025993536692112684\n","====== 144 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9789217202970297\n","Train Loss : 0.00011656838614726439\n","\n","Valid Acc : 0.8608\n","Valid Loss : 0.0026077700313180685\n","====== 145 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9784576113861386\n","Train Loss : 0.0002721176715567708\n","\n","Valid Acc : 0.8571\n","Valid Loss : 0.002593593206256628\n","====== 146 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9778774752475248\n","Train Loss : 0.00024034257512539625\n","\n","Valid Acc : 0.8585\n","Valid Loss : 0.0026275364216417074\n","====== 147 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9778001237623762\n","Train Loss : 0.0001499700447311625\n","\n","Valid Acc : 0.8589\n","Valid Loss : 0.0026404738891869783\n","====== 148 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9773746905940595\n","Train Loss : 0.0001643062714720145\n","\n","Valid Acc : 0.8606\n","Valid Loss : 0.0026319921016693115\n","====== 149 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9789603960396039\n","Train Loss : 0.00017968677275348455\n","\n","Valid Acc : 0.8613\n","Valid Loss : 0.0025901454500854015\n","====== 150 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9800046410891089\n","Train Loss : 0.00017781814676709473\n","\n","Valid Acc : 0.8597\n","Valid Loss : 0.0026165256276726723\n","====== 151 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9800046410891089\n","Train Loss : 0.00019795911794062704\n","\n","Valid Acc : 0.8617\n","Valid Loss : 0.002612520707771182\n","====== 152 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9787283415841584\n","Train Loss : 0.00012109289673389867\n","\n","Valid Acc : 0.8621\n","Valid Loss : 0.0025921608321368694\n","====== 153 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9797339108910891\n","Train Loss : 0.0001756579295033589\n","\n","Valid Acc : 0.8619\n","Valid Loss : 0.002618168480694294\n","====== 154 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9808168316831684\n","Train Loss : 0.00019339997379574925\n","\n","Valid Acc : 0.8618\n","Valid Loss : 0.0026202406734228134\n","====== 155 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9805074257425742\n","Train Loss : 0.0001464844826841727\n","\n","Valid Acc : 0.8591\n","Valid Loss : 0.0026614603120833635\n","====== 156 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9805074257425742\n","Train Loss : 0.00022510957205668092\n","\n","Valid Acc : 0.8602\n","Valid Loss : 0.0026929073501378298\n","====== 157 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9790764232673267\n","Train Loss : 0.0002145278558600694\n","\n","Valid Acc : 0.8612\n","Valid Loss : 0.0026872556190937757\n","====== 158 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9803527227722773\n","Train Loss : 0.00010859354370040819\n","\n","Valid Acc : 0.8623\n","Valid Loss : 0.002667191671207547\n","====== 159 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9807394801980198\n","Train Loss : 0.00025125182582996786\n","\n","Valid Acc : 0.8591\n","Valid Loss : 0.0027036075480282307\n","====== 160 epoch of 160 ======\n","\n","====== 100 Step of 196 ======\n","Train Acc : 0.9786123143564357\n","Train Loss : 0.0001297172566410154\n","\n","Valid Acc : 0.8619\n","Valid Loss : 0.002690679859369993\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[{"file_id":"https://github.com/CryptoSalamander/pytorch_paper_implementation/blob/master/resnet/resnet_cifar10.ipynb","timestamp":1723307444631}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}